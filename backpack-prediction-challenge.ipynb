{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Kaggle Playground Series - Season 5, Episode 2**","metadata":{}},{"cell_type":"markdown","source":"## **Objective**  \nIn this episode we are given the task of predicting the price of backpacks given various attributes.  \nSubmissions are scored on the root mean squared error.  \n\nRMSE is defined as:\n\n$$\n\\textrm{RMSE} =  \\left( \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\widehat{y}_i)^2 \\right)^{\\frac{1}{2}}\n$$\n\n## **Data**  \nThe dataset for this competition is generated from a deep learning model trained on the [Student Bag Pric Preditions Dateset](https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset/data)\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport optuna\nfrom optuna.study import MaxTrialsCallback\n\nprint('Libaires imported')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s5e2/train.csv')\ndf_train_ex = pd.read_csv('/kaggle/input/playground-series-s5e2/training_extra.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e2/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s5e2/sample_submission.csv')\nprint('Data imported')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Initial Observations**  \n","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_ex","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_combined = pd.concat([df_train, df_train_ex], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_combined","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = df_train_combined","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Summary of Datasets**","metadata":{}},{"cell_type":"code","source":"missing_values_train = pd.DataFrame({'Feature': df_train.columns,\n                              '[TRAIN] No. of Missing Values': df_train.isnull().sum().values,\n                              '[TRAIN] % of Missing Values': ((df_train.isnull().sum().values)/len(df_train)*100)})\n\nmissing_values_test = pd.DataFrame({'Feature': df_test.columns,\n                             '[TEST] No.of Missing Values': df_test.isnull().sum().values,\n                             '[TEST] % of Missing Values': ((df_test.isnull().sum().values)/len(df_test)*100)})\n\nunique_values = pd.DataFrame({'Feature': df_train.columns,\n                              'No. of Unique Values[FROM TRAIN]': df_train.nunique().values})\n\nfeature_types = pd.DataFrame({'Feature': df_train.columns,\n                              'DataType': df_train.dtypes})\n\ndf_summary = pd.merge(missing_values_train, missing_values_test, on='Feature', how='left')\ndf_summary = pd.merge(df_summary, unique_values, on='Feature', how='left')\ndf_summary = pd.merge(df_summary, feature_types, on='Feature', how='left')\n\ndf_summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_duplicates = df_train.duplicated().sum()\ntest_duplicates = df_test.duplicated().sum()\n\nprint(f\"Number of duplicate rows in df_train: {train_duplicates}\")\nprint(f\"Number of duplicate rows in df_test: {test_duplicates}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Dataset Observations**\n\n### **Shape**\nTraining Data: 3,994,318rows × 11 columns  \nTest Data: 200,000 rows × 10 columns\n\n### **Missing Values**\nSeveral features contain missing values in both the training and test sets:  \n\n**Training set**\n- Brand: ~3.17%\n- Material:  ~2.78%\n- Size: ~2.20%\n- Laptop Compartment: ~2.47%\n- Waterproof: ~2.36%\n- Style: ~2.61%\n- Color: ~3.35%\n- Weight Capacity (kg): ~0.05%\n\n**Test set**\n- Brand: ~3.11%\n- Material: ~2.81%\n- Size: ~2.19%\n- Laptop Compartment: ~2.48%\n- Waterproof: ~2.41%\n- Style: ~2.58%\n- Color: ~3.39%\n- Weight Capacity (kg): ~0.04%\n\n\n### **Feature Breakdown**\n\n- **ID:** A unique identifier for each backpack.\n- **Brand, Material, Size, Style:** Categorical variables.\n- **Compartments:** Numeric, range from 1 to 10.\n- **Laptop Compartment & Waterproof:** Binary categorical.\n- **Color:** 6 unique values, with missing data.\n- **Weight Capacity (kg):** Numerical.\n- **Price:** The target variable in the training set.","metadata":{}},{"cell_type":"markdown","source":"## **EDA**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.histplot(df_train[\"Price\"], bins=50, kde=True)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Price (Target Variable)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.heatmap(df_train.isnull(), cbar=False, cmap=\"viridis\")\nplt.title(\"Missing Values Heatmap\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Brand\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Brand\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Material\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Material\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Size\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Size\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Color\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Color\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Style\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Style\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Laptop Compartment\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Laptop Compartment\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x=\"Waterproof\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Waterproof\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.scatterplot(x=\"Compartments\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Compartments\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.scatterplot(x=\"Weight Capacity (kg)\", y=\"Price\", data=df_train)\nplt.title(\"Price Distribution by Weight Capacity (kg)\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Data Imputation**","metadata":{}},{"cell_type":"markdown","source":"**Brand, Material, Size, Laptop Compartment, Waterproof, Style, Color**\nDue to the small catagorical types and little to interpret from the plots from the EDA the best approach for the missing values would be to use mode imputation\n\n**Weight Capacity (kg)**\nHere I will use the median value for the imputation since we are working with a continuous numerical variable.","metadata":{}},{"cell_type":"code","source":"categorical_features = [\"Brand\", \"Material\", \"Size\", \"Laptop Compartment\", \"Waterproof\", \"Style\", \"Color\"]\nnumerical_features = [\"Weight Capacity (kg)\"]\n\nfor col in categorical_features:\n    df_train[col].fillna(df_train[col].mode()[0], inplace=True)\n    df_test[col].fillna(df_test[col].mode()[0], inplace=True)\n\nfor col in numerical_features:\n    df_train[col].fillna(df_train[col].median(), inplace=True)\n    df_test[col].fillna(df_test[col].median(), inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Encoding Categorical Features**","metadata":{}},{"cell_type":"code","source":"# One-Hot Encoding for categorical variables\none_hot_features = [\"Brand\", \"Material\", \"Size\", \"Style\", \"Color\"]\ndf_train = pd.get_dummies(df_train, columns=one_hot_features)\ndf_test = pd.get_dummies(df_test, columns=one_hot_features)\n\n\n# Label Encoding for binary variables\nbinary_features = [\"Laptop Compartment\", \"Waterproof\"]\nfor col in binary_features:\n    le = LabelEncoder()\n    df_train[col] = le.fit_transform(df_train[col])\n    \nfor col in binary_features:\n    le = LabelEncoder()\n    df_test[col] = le.fit_transform(df_test[col])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Baseline Model - Linear Regression**","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(columns=[\"Price\", \"id\"])\ny = df_train[\"Price\"]\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred))\nprint(\"Baseline RMSE:\", rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **LightGBM Model**","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(columns=[\"Price\", \"id\"])\ny = df_train[\"Price\"]\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlgb_model = lgb.LGBMRegressor(\n    n_estimators=1000, \n    learning_rate=0.05, \n    max_depth=10, \n    num_leaves=31\n)\nlgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric=\"rmse\",\n    callbacks=[lgb.early_stopping(100)]\n)\n\ny_pred = lgb_model.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred))\nprint(\"LightGBM RMSE:\", rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2000),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.1),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 80),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 50),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-3, 10.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 10.0),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n    }\n    \n\n    model = lgb.LGBMRegressor(**params, random_state=42)\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(10)],\n    )\n    \n    y_pred = model.predict(X_valid)\n    \n\n    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    \n    return rmse","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = MaxTrialsCallback(5)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\n\n\nprint(\"Best Hyperparameters:\", study.best_params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = study.best_params\n\nbest_lgb_model = lgb.LGBMRegressor(**best_params, random_state=42)\n\nbest_lgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric=\"rmse\",\n    callbacks=[lgb.early_stopping(100)],\n)\n\ny_pred = best_lgb_model.predict(X_valid)\n\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred))\nprint(\"Optimized LightGBM RMSE:\", rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Submission File**","metadata":{}},{"cell_type":"markdown","source":"X_test = df_test.drop(columns=[\"id\"])\n\ny_test_pred = lr.predict(X_test)\n\n\n# Create submission file\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"Price\": y_test_pred\n})\n\n# Step 4: Save to CSV (Ensure No Index)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file saved: submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-02-12T18:15:05.462116Z","iopub.execute_input":"2025-02-12T18:15:05.462608Z","iopub.status.idle":"2025-02-12T18:15:05.878056Z","shell.execute_reply.started":"2025-02-12T18:15:05.462565Z","shell.execute_reply":"2025-02-12T18:15:05.876803Z"}}},{"cell_type":"markdown","source":"# # Rename Columns to Force LightGBM to Recognize Cleaned Names\nX_train.columns = X_train.columns.str.replace(\" \", \"_\").str.replace(\"(\", \"\").str.replace(\")\", \"\")\nX_valid.columns = X_valid.columns.str.replace(\" \", \"_\").str.replace(\"(\", \"\").str.replace(\")\", \"\")\n\n# Confirm Feature Names\nprint(\"Feature Names in X_train:\", X_train.columns.tolist())Ensure feature names are properly formatted in X_train, X_valid, and X_test\nX_train.columns = X_train.columns.str.replace(\" \", \"_\")\nX_valid.columns = X_valid.columns.str.replace(\" \", \"_\")\nX_test.columns = X_test.columns.str.replace(\" \", \"_\")\n\n# Verify feature names\nprint(\"Feature Names in X_train:\", X_train.columns.tolist())# Ensure feature names are properly formatted in X_train, X_valid, and X_test\nX_train.columns = X_train.columns.str.replace(\" \", \"_\")\nX_valid.columns = X_valid.columns.str.replace(\" \", \"_\")\nX_test.columns = X_test.columns.str.replace(\" \", \"_\")\n\n# Verify feature names\nprint(\"Feature Names in X_train:\", X_train.columns.tolist())X_test = df_test.drop(columns=[\"id\"])\nX_test = X_test.reindex(columns=X_train.columns, fill_value=0)\ny_test_pred = lgb_model.predict(X_test)\n\n# Create Submission File\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"Price\": y_test_pred\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file saved: submission.csv\")\n","metadata":{}}]}